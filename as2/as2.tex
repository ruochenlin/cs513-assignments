\title{CS 513 Assignment 2}
\author{Ruochen Lin}
\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{setspace}
% \setmonofont{hack}
\begin{document}
\maketitle
\section{}
\subsection{}
The characteristic equation of matrix $A$ is 
\begin{equation}\begin{split} (1-\lambda)(2-\lambda)+2 = 0,\end{split}\nonumber\end{equation} 
solving which will give us the two eigenvalues of $A$:
\begin{equation}\begin{split} \lambda = \frac{3\pm\sqrt{7}i}2.\end{split}\nonumber\end{equation} 
They both have the same `length' in the complex plane:
$$\abs{\lambda}=\sqrt{(\frac32)^2+(\frac{\sqrt{7}}2)^2}=\sqrt{\frac{16}4}=2,$$
which is the spectral radius of $A$.

\subsection{}
The $l_1$- and $l_{\infty}$-norms of $A$ are easy to find:
\begin{equation}\begin{split}\norm{A}_1=2+1=3,\,\,\norm{A}_\infty=2+2=4.\end{split}\nonumber\end{equation}
Finding the $l_2$-norm, on the other hand, goes hand-in-hand with the SVD of $A$; so we give the result below, but postpone the corresponding calculation to the next subsection:
\begin{equation}\begin{split} \norm{A}_2=2\sqrt{2}.\end{split}\nonumber\end{equation} 

\subsection{}
\begin{equation}\begin{split} 
A^TA&=\begin{bmatrix} 2 & 1\\-2&1 \end{bmatrix}\begin{bmatrix} 2&-2\\1&1\end{bmatrix}=\begin{bmatrix} 5&-3\\-3&5\end{bmatrix},\\
AA^T&=\begin{bmatrix} 2&-2\\1&1\end{bmatrix}\begin{bmatrix} 2 & 1\\-2&1 \end{bmatrix}=\begin{bmatrix} 8 & 0\\0 & 2\end{bmatrix}.
\end{split}\nonumber\end{equation}
Schur decomposing $A^TA$ and $AA^T$ gives us the left and right singular matrices of $A$, $U$ and $V$:
\begin{equation}\begin{split} A^TA&=V\Lambda V^T,\\AA^T&=U\Lambda U^T.\end{split}\nonumber\end{equation} 
Solving the characteristic equation of $A^TA$, 
\begin{equation}\begin{split} (5-\lambda)^2-9=0,\end{split}\nonumber\end{equation}
gives us the eigenvalues of $A^TA$, 
\begin{equation}\begin{split} \lambda_1=2,\,\,\lambda_2=8,\end{split}\nonumber\end{equation} 
and the corresponding normalised eigenvectors:
\begin{equation}\begin{split} v_1 &= \begin{bmatrix} \frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2\end{bmatrix}, \\
 v_2 &= \begin{bmatrix}-\frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2\end{bmatrix}.
\end{split}\nonumber\end{equation} 
Thus we have:
\begin{equation}\begin{split} \Lambda&=\begin{bmatrix} 2 & 0\\ 0 & 8\end{bmatrix}, \\
V&=\begin{bmatrix} \frac{\sqrt{2}}2 & -\frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2 & \frac{\sqrt{2}}2\end{bmatrix}.
\end{split}\nonumber\end{equation} 
The square roots of the eigenvalues gives us the sigular values of $A$: 
\begin{equation}\begin{split} \sigma_1=\sqrt{\lambda_1}=\sqrt{2},\,\,\sigma_2=\sqrt{\lambda_2} = 2\sqrt{2}.\end{split}\nonumber\end{equation}
The largest of the two ($\sigma_2=2 \sqrt{2}$) is also the $l_2$-norm of $A$.\\[0.3cm]
In order to find $U$, we can decompose $AA^T$ in the same fashion; however, since we have figured out $\Lambda$ and $V$, an easier way of finding $U$ would be recognizing that:
\begin{equation}\begin{split} 
Av_1 &= \begin{bmatrix} 2 & -2 \\ 1 & 1\end{bmatrix}\begin{bmatrix} \frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2\end{bmatrix} = \begin{bmatrix} 0 \\ \sqrt{2}\end{bmatrix}\\
&=\sqrt{2}\,u_1,\\
Av_2 &= \begin{bmatrix} 2 & -2 \\ 1 & 1\end{bmatrix}\begin{bmatrix} \frac{-\sqrt{2}}2 \\ \frac{\sqrt{2}}2\end{bmatrix} = \begin{bmatrix}-2\sqrt{2}\\0\end{bmatrix}\\
&=2\sqrt{2}\,u_2.
\end{split}\nonumber\end{equation} 
Hence 
\begin{equation}\begin{split} U = \begin{bmatrix} 0 & -1 \\ 1 & 0\end{bmatrix} \end{split}\nonumber\end{equation} and
\begin{equation}\begin{split} A = U\Sigma V^T = U\Lambda^{\frac12}V^T\end{split},\nonumber\end{equation}
where $\Sigma$ is defined as a diagonal matrix with diagonal entries $\Sigma_{ii} = \sigma_i$.\\[0.3cm]
Indeed, we find that 
\begin{equation}\begin{split} 
AV &= \begin{bmatrix} 2 & -2\\1 & 1\end{bmatrix}\begin{bmatrix} \frac{\sqrt{2}}2 & -\frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2 & \frac{\sqrt{2}}2 \end{bmatrix} = 
\begin{bmatrix} 0 & -\sqrt{2} \\ \sqrt{2} & 0\end{bmatrix}\\
&= U\Sigma = \begin{bmatrix} 0 & -1\\1 & 0\end{bmatrix}\begin{bmatrix} \sqrt{2} & 0\\0 & 2 \sqrt{2} \end{bmatrix}   
\end{split}\nonumber\end{equation} 
and that
\begin{equation}\begin{split} 
A^TU &= \begin{bmatrix} 2 &1 \\ -2 & 1\end{bmatrix} \begin{bmatrix} 0 & -1\\1 & 0\end{bmatrix} = \begin{bmatrix} 1 & -2\\ 1 & 2\end{bmatrix} \\
&=V\Sigma = \begin{bmatrix} \frac{\sqrt{2}}2 & -\frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2 & \frac{\sqrt{2}}2 \end{bmatrix} \begin{bmatrix}\sqrt{2} & 0\\0 & 2\sqrt{2}\end{bmatrix}.
\end{split}\nonumber\end{equation} 

\section{}
\subsection{}
After playing around in \texttt{MATLAB}, (code can be found in Appendix B,) my finding is that \\[0.4cm] 
\textbf{Theorem} Given a symmetric matrix $A$, $(\lambda, v)$ or $(-\lambda, v)$ is an eigenpair of $A^TA=A^2$ if and only if $(\lambda^2, v)$ is an eigenpair of $A^T=A$.\\[0.3cm]
We might be tempted to state a stronger form of the theorem, \textit{i.e.} $(\lambda,v)$ is an eigenpair of $A$ if and only if $(\lambda^2,v)$ is one for $A^2$. However, if we set $\lambda = 2$ and $$A = \begin{bmatrix} 1 & 0 \\ 0 & -2\end{bmatrix},$$
then apparently $(\lambda^2=4,\begin{bmatrix} 0 & 1\end{bmatrix}^T)$ is an eigenpair of $$A^2 = \begin{bmatrix} 1 &0\\0 &4\end{bmatrix}; $$ however, $\lambda=2$ is not an eigevalue of $A$, and thus the `if' part of the stronger form of the theorem is false. We hence have to live with the more `verbose' version of the theorem.
\subsection{}
\textit{Proof:}\\[0.4cm]
If 
\begin{equation}\begin{split} Av = \lambda v \,\, \text{or}\,\, Av = -\lambda v , \end{split}\nonumber\end{equation}
then
\begin{equation}\begin{split} A^2v=A(Av)=\pm \lambda Av = \lambda^2v;\end{split}\nonumber\end{equation}
in other words, $Av=\lambda v$ or $Av=-\lambda v$ only if $A^2v = \lambda^2v$.\\[0.3cm]
To prove the `if' part, since $A$ is symmetric, it can be decomposed as the following:
\begin{equation}\begin{split} A = Q\Lambda Q^T\end{split}\nonumber\end{equation}
with $\Lambda$ being a digonal matrix constructed from the eigenvalues of $A$ and $Q$ being orthogonal. We can also express $A^2$ in terms of $\Lambda$ and $Q$:
$$A^2 = AA = (Q\Lambda Q^T)(Q\Lambda Q^T) = Q\Lambda^2 Q^T.$$
We recognise that the equation above happens to be the Schur decomposition of $A^2$, because $\Lambda^2$ is also diagonal. If $\lambda^2$ is one of the eigenvalues of $A^2$, then it must be one of the diagonal terms of $\Lambda^2$, say $\lambda^2 = (\Lambda^2)_{ii} = (\Lambda_{ii})^2$, $$\Rightarrow\Lambda_{ii}= \pm\lambda. $$
This is to say that, if $\lambda^2$ is the eigenvalue of $A^2$ that corresponds to the eigenvector of $q_i$, then either $\lambda$ or $-\lambda$ must also be the eigenvalue of $A$ that corresponds to eigenvector $q_i$. Hence we finished the `if' part of proof.

\subsection{}
Since $$\norm{A}_2 = \sqrt{\max\sigma(A^TA)}$$ and $$\lambda_{A} = \pm\sqrt{\lambda_{A^TA}},$$ the $l_2$-norm of a real symmetric matrix $A$ is equal to the element in its spectrum with the largest absolute vaue, \textit{i.e.} $$\norm{A}_2 = \max_{\lambda}\abs{\lambda},\,\,\lambda\in\sigma(A).$$ 
Apply this result to $$A = \begin{bmatrix} -8 & 144\\144 & -92\end{bmatrix}, $$ 
whose eigenvalues are $$\lambda_1 = -200,\,\,\lambda_2 = 100,$$
we predict the $l_2$-norm of $A$ to be 
$$\norm{A}_2 = \max\{\abs{-200},\,\abs{100}\} = 200,$$
which is indeed the $l_2$-norm of $A$. (Can be verified with \texttt{MATLAB}.)

\subsection{}
The theorem from preceeding parts does not hold for more general matrices; for example, given 
\begin{equation}\begin{split} A &= \begin{bmatrix} 0 & -2 \\ 1 & 0\end{bmatrix},\\
\sigma(A) &= \{-\sqrt{2} ,\, \sqrt{2} \};
  \end{split}\nonumber\end{equation} 
following the previous theorem we would predict that 
$$\norm{A}_2 = \sqrt{2}. $$
However, if we calculate the $l_2$-norm in the canonical way, we would find that 
\begin{equation}\begin{split} \sigma(A^TA) &= \{1,\,4\},\\ 
\norm{A}_2 &= \sqrt{4} = 2\\&\neq\sqrt{2},  
\end{split}\nonumber\end{equation}
which contradicts the theorem. 
\subsection{}
Following the discussion above, we can say that the sigular values of a symmetric matrix are equal to the absolute values of its eigenvalues.

\section{}
\subsection{}
If $A$ is a non-singular matrix and $(\lambda, v)$ is one of its eigenpairs, \textit{i.e.}
\begin{equation}\begin{split} Av = \lambda v, \end{split}\nonumber\end{equation} then 
\begin{equation}\begin{split} A^{-1}Av &= (A^{-1}A)v = v \\ &= A^{-1}(Av) = \lambda A^{-1}v \end{split}\nonumber\end{equation}
$$\Rightarrow A^{-1}v = \frac1{\lambda}v.$$
In addition, $A$ is also the inverse of $A^{-1}$, so $A^{-1}v=\frac1{\lambda}v$ will lead to $Av = \lambda v$ as well. Hence we proved that $Av=\lambda v$ if and only if $A^{-1 }v = \frac1{\lambda}v$.

\subsection{}
Given that $A$ is non-singular, 
$$\norm{A^{-1}}_2 =\frac1{\min\{\sigma\}},\,\sigma \geqslant 0, \,\sigma^2\in\sigma(A^TA).$$
\textit{Proof:}\\[0.3cm]
Given an SVD of $A$:
$$A = U\Sigma V^T,$$ then
$$A^{-1} = V\Sigma^{-1}U^T,$$
where $(\Sigma^{-1})_{ij} = 0$ if $i\neq j$ and $(\Sigma^{-1})_{ii} = \frac1{\Sigma_{ii}}$.\\[0.3cm]
It can easily be verified that $\Sigma^{-1}$ and $A^{-1}$ expressed as such are exactly the inverses of $\Sigma$ and $A$ respectively, and thus $\{\sigma_i = \Sigma_{ii}\}$ and $\{\sigma^{-1}_i = \Sigma^{-1}_{ii}\}$ are the sets of sigular values of $A$ and $A^{-1}$. The $l_2$-norm of $A^{-1}$ can be expressed as the following:
\section{Evidence of the theorem in Problem 2}
The following code was used to 
% TODO: provide evidence to the theorem
$$\norm{A^{-1}}_2 = \max_{\sigma_i^{-1}}{\abs{\sigma^{-1}_i}} = \max_{\sigma_i}\frac1{\sigma_i}=\frac1{\min\{\sigma_i\}}.$$
Consider both $\{\sigma\}$ and $\{\sigma_i\}$ represent the set of singular values of $A$, we've proved the theorem.

\section{}
$$Z = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6\\ 7 & 8 & 7 \\ 4 & 2 & 3 \\ 4 & 2 & 2\end{bmatrix} $$
Since $Z$ is a $5\times3$ rectangular matrix, it can be QR-decomposed into the product of a $5\times5$ orthogonal matrix $Q$ and a $5\times 3$ upper triangular matrix $R$: $$Z_{5\times3} = Q_{5\times5}R_{5\times3}.$$
The $l_2$-norm (abbreviated as simply norm below) of the first column of $Z$, denoted as $x_1$, is $$\norm{x_1}_2 = \sqrt{1^2+4^2+7^2+4^2+4^2}=\sqrt{98} = 7 \sqrt{2}; $$ thus $$y_1 = \begin{bmatrix} 7\sqrt{2}\\0\\0\\0\\0\end{bmatrix},$$ $$ \norm{x_1-y_1} = \sqrt{196-14\sqrt{2}}\approx13.2741, $$ and $$w_1 = \frac{x_1-y_1}{\norm{x_1-y_1}} = \begin{bmatrix} \frac{1-7 \sqrt{2}}{\sqrt{196-14\sqrt{2}}} \\ \frac4{\sqrt{196-14\sqrt{2}}} \\ \frac7{\sqrt{196-14\sqrt{2}}} \\ \frac4{\sqrt{196-14\sqrt{2}}} \\ \frac4{\sqrt{196-14\sqrt{2}}}\end{bmatrix}\approx \begin{bmatrix} -0.6704 \\ 0.3013 \\ 0.5273 \\ 0.3013 \\ 0.3013\end{bmatrix}. $$
We can construct the first Householder matrix as the following:
\begin{equation}\begin{split} H_1 &= I - 2ww^T\\
&\approx 
\begin{bmatrix} 
0.1010 & 0.4041 & 0.7071 & 0.4041 & 0.4041 \\ 
0.4041 & 0.8184 & -0.3178 & -0.1816 & -0.1816 \\
0.7071 & -0.3178 & 0.4438 & -0.3178 & -0.3178 \\
0.4041 & -0.1816 & -0.3178 & 0.8184 & -0.1816 \\
0.4041 & -0.1816 & -0.3178 & -0.1816 & 0,8184
\end{bmatrix}  
\end{split}\nonumber\end{equation} 
Multiply $Z$ by $H_1$ on the left, we have:
$$H_1Z \approx \begin{bmatrix} 
9.8995 & 9.4954 & 9.6975 \\
0 & 1.6311 & 2.9897 \\
0 & 2.1044 & 1.7320 \\
0 & -1.3689 & -0.0103 \\
0 & -1.3689 & -1.0103
\end{bmatrix} $$
Following this strategy we have:
\begin{equation}\begin{split} 
x_2 &\approx 
\begin{bmatrix} 
9.4954 \\ 1.6311 \\ 2.1044 \\ -1.3689 \\  -1.3689
\end{bmatrix},\,\,
y_2 \approx \begin{bmatrix} 
9.4954 \\ 3.2919 \\ 0\\ 0\\ 0
\end{bmatrix},\,\,
w_2 \approx \begin{bmatrix} 0 \\ -0.5023 \\ 0.6364 \\ -0.4140 \\ -0.4140\end{bmatrix},\\
H_2 &= I - 2w_2w_2^T\\
&\approx 
\begin{bmatrix} 
1 & 0 & 0 & 0 & 0 \\
0 & 0.4955 & 0.6393 & -0.4158 & -0.4158 \\
0 & 0.6393 & 0.1900 & 0.5269 & 0.5269 \\
0 & -0.4158 & 0.5269 & 0.6572 & -0.3428 \\
0 & -0.4158 & 0.5269 & -0.3428 & 0.6572
\end{bmatrix},\\
H_2H_1Z &\approx
\begin{bmatrix} 9.8995 & 9.4952 & 9.6975 \\
0 & 3.2919 & 3.0129 \\
0 & 0 & 1.7026 \\
0 & 0 & 0.0089 \\
0 & 0 & -0.9911 
\end{bmatrix} 
\end{split}\nonumber\end{equation} 
and that 
\begin{equation}\begin{split} 
x_3 &\approx
\begin{bmatrix} 9.6975 \\ 3.0129 \\ 1.7026 \\ 0.0089 \\ -0.9911\end{bmatrix},\,\,
y_3 \approx \begin{bmatrix} 
9.6975 \\ 3.0129 \\ 1.9701 \\ 0 \\ 0
\end{bmatrix},\,\,
w_3 \approx \begin{bmatrix} 0 \\ 0 \\ -0.2606 \\ 0.0086 \\ -0.9654\end{bmatrix}, \\
H_3 &= I - 2w_3w_3^T \approx 
\begin{bmatrix} 
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0.8642 & 0.0045 & -0.5031 \\
0 & 0 & 0.0045 & 0.9999 & 0.0167 \\
0 & 0 & -0.5031 & 0.0167 & -0.8641
\end{bmatrix},\\
R &= H_3H_2 H_1Z \approx
\begin{bmatrix} 
9.8995 & 9.4954 & 9.6975 \\
0 & 3.2919 & 3.0129 \\
0 & 0 & 1.9701 \\
0 & 0 & 0\\
0 & 0 & 0
\end{bmatrix},\\
Q &= (H_3H_2H_1)^T = H_1^TH_2^TH_3^T\\
&\approx
\begin{bmatrix} 
0.1010 & 0.3162 & 0.5420 & 0.3408 & -0.6928 \\
0.4041 & 0.3534 & 0.5162 & -0.5730 & 0.3422\\
0.7071 & 0.3906 & -0.5248 & 0.2684 & 0.0028\\
0.0041 & -0.5580 & 0.3871 & 0.5006 & 0.3534\\
0.4041 & -0.5580 & -0.1204 & -0.4825 & -0.5273
\end{bmatrix}.
\end{split}\nonumber\end{equation} 
And we find that $\norm{Z-QR}_2 \sim 10^{-15}$, implying that $Z\approx QR$, just as expected.

\section{}
\subsection{}
Suppose $Y = HX=(I-2ww^T)X=X-2ww^TX$ is our target matrix to compute, and ${x_i}$ and ${y_i}$ are the $i$th column of matrices $X$ and $Y$ respectively, then
\begin{equation}\begin{split} 
y_i = x_i -2(w^Tx_i)w.
\end{split}\nonumber\end{equation} 
If we compute $y_i$s in this fashion, then the computation of the inner product between $w$ and $x_i$ requires $m$ multiplications and $m-1$ additions, incurring a cost of $O(m)$; multiplying $w$ by $-2w^Tx_i$ needs another $m$ multiplications, and adding the resulting vector to $x_i$ requires $m$ additions. In general, calculating $y_i$ needs $O(m)+O(m) + O(m) = O(m)$ elemental operations, and there are $m$ of them to calculate; thus getting the whole matrix of $Y$ following this strategy would be of $O(m^2)$ complexity.

\subsection{}
The strategy we propose is that, given a linear system $$Ax = b,$$ and assuming that $A$ can be QR-factorized into $$A = QR,$$ 
\begin{equation}\begin{split}
\Rightarrow Rx&=Q^Tb=H_mH_{m-1}\dots H_1b \\
&= (I-2w_mw_m^T)(I-2w_{m-1}w_{m-1}^T)\dots(I-2w_1w_1^T)b :=\tilde b.
\end{split}\nonumber\end{equation} 
Thus while we are QR-factorizing $A$ with matrices $(I-2w_iw_i^T)$, we multiply the vector on the right hand side by the transformation matrix as well, (this is also of $O(m^2)$ complexity, thus will not change the asymptotic behaviour of the algorithm;) in the $k$th step we only need to multiply $(m-k+2)$ columns ($m-k+1$ on the left hand side, $1$ on the right hand side) by the transformation matrix; however, the comlexity of QR-factorization is still of $O(m^3)$ overall. At the end of the factorization, we should end up with $$Rx=\tilde b,$$ the solution of which is also the solution of the original problem. Since $R$ is upper triangular, we can solve for each component of $x$ bottom up, which add a complexity of $O(m^2)$. In general, we expect the algorithm to solve the linear system within $O(m^3)$ time.

\subsection{}
We generated two $5\times5$ and two $5\times1$ matrices with random entries as our test $A$s and $b$s; by calculating the $l_2$-norm of $Ax-b$, we verified our algorithm to be correct. (Please refer to the attached diary.)

\subsection{}
As stated in $5.2$, we expect our algorithm to have the time complexity of $O(m^3)$; to verify this, we tested our script with random matrices of $2\times2$ all the way up to $21\times21$; by fitting the corresponding number of operations used against the dimensionality, we find that the leading term of complexity is $2m^3$, just as expected. If we use quartic model (\texttt{'poly4'} in \texttt{MATLAB} terminology), then the coefficient of the $m^4$ term will be negligible. 

\newpage
\appendix{}
\section{\texttt{MATLAB} code that verifies results in Problem 1}
The script that verifies our result from Problem 1 is the following:\\[0.4cm]
\texttt{A = {[2, -2; 1, 1]};\\
rho = max(abs(eig(A)))\\
norm\_1 = norm(A, 1)\\
norm\_2 = norm(A, 2)\\
norm\_inf = norm(A, Inf)\\
{[U, Sigma, V]} = svd(A)}\\[0.4cm]
And the output is:\\
\texttt{rho =\\
\\
\ \ \ \ \ 2
\\
\\
norm\_1 =\\
\\
\ \ \ \ \ 3
\\
\\
norm\_2 =\\
\\
\ \ \ \ 2.8284
\\
\\
norm\_inf =\\
\\
\ \ \ \ \ 4\\
\\
\\
U =\\
\\
\ \ \ -1.0000\ \ \ -0.0000\\
\ \ \ -0.0000\ \ \ \ 1.0000\\
\\
\\
Sigma =\\
\\
\ \ \ \ 2.8284\ \ \ \ \ \ \ \ \ 0\\
\ \ \ \ \ \ \ \ \ 0\ \ \ \ 1.4142\\
\\
\\
V =\\
\\
\ \ \ -0.7071\ \ \ \ 0.7071\\
\ \ \ \ 0.7071\ \ \ \ 0.7071}\\[0.4cm]
Note that the numerical values of $\rho$ and the norms corroborate the results we got in Problem 1. The result from \texttt{MATLAB svd} function minorly differs from our result in Problem 1; however, since both the singular values and the corresponding left/right singular vectors are indentical, our result in Problem 1 must be correct as well.




\end{document}

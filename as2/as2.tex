\title{CS 513 Assignment 2}
\author{Ruochen Lin}
\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{setspace}
% \setmonofont{hack}
\begin{document}
\maketitle
\section{}
\subsection{}
The characteristic equation of matrix $A$ is 
\begin{equation}\begin{split} (1-\lambda)(2-\lambda)+2 = 0,\end{split}\nonumber\end{equation} 
solving which will give us the two eigenvalues of $A$:
\begin{equation}\begin{split} \lambda = \frac{3\pm\sqrt{7}i}2.\end{split}\nonumber\end{equation} 
They both have the same `length' in the complex plane:
$$\abs{\lambda}=\sqrt{(\frac32)^2+(\frac{\sqrt{7}}2)^2}=\sqrt{\frac{16}4}=2,$$
which is the spectral radius of $A$.

\subsection{}
The $l_1$- and $l_{\infty}$-norms of $A$ are easy to find:
\begin{equation}\begin{split}\norm{A}_1=2+1=3,\,\,\norm{A}_\infty=2+2=4.\end{split}\nonumber\end{equation}
Finding the $l_2$-norm, on the other hand, goes hand-in-hand with the SVD of $A$; so we give the result below, but postpone the corresponding calculation to the next subsection:
\begin{equation}\begin{split} \norm{A}_2=2\sqrt{2}.\end{split}\nonumber\end{equation} 

\subsection{}
\begin{equation}\begin{split} 
A^TA&=\begin{bmatrix} 2 & 1\\-2&1 \end{bmatrix}\begin{bmatrix} 2&-2\\1&1\end{bmatrix}=\begin{bmatrix} 5&-3\\-3&5\end{bmatrix},\\
AA^T&=\begin{bmatrix} 2&-2\\1&1\end{bmatrix}\begin{bmatrix} 2 & 1\\-2&1 \end{bmatrix}=\begin{bmatrix} 8 & 0\\0 & 2\end{bmatrix}.
\end{split}\nonumber\end{equation}
Schur decomposing $A^TA$ and $AA^T$ gives us the left and right singular matrices of $A$, $U$ and $V$:
\begin{equation}\begin{split} A^TA&=V\Lambda V^T,\\AA^T&=U\Lambda U^T.\end{split}\nonumber\end{equation} 
Solving the characteristic equation of $A^TA$, 
\begin{equation}\begin{split} (5-\lambda)^2-9=0,\end{split}\nonumber\end{equation}
gives us the eigenvalues of $A^TA$, 
\begin{equation}\begin{split} \lambda_1=2,\,\,\lambda_2=8,\end{split}\nonumber\end{equation} 
and the corresponding normalised eigenvectors:
\begin{equation}\begin{split} v_1 &= \begin{bmatrix} \frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2\end{bmatrix}, \\
 v_2 &= \begin{bmatrix}-\frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2\end{bmatrix}.
\end{split}\nonumber\end{equation} 
Thus we have:
\begin{equation}\begin{split} \Lambda&=\begin{bmatrix} 2 & 0\\ 0 & 8\end{bmatrix}, \\
V&=\begin{bmatrix} \frac{\sqrt{2}}2 & -\frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2 & \frac{\sqrt{2}}2\end{bmatrix}.
\end{split}\nonumber\end{equation} 
The square roots of the eigenvalues gives us the sigular values of $A$: 
\begin{equation}\begin{split} \sigma_1=\sqrt{\lambda_1}=\sqrt{2},\,\,\sigma_2=\sqrt{\lambda_2} = 2\sqrt{2}.\end{split}\nonumber\end{equation}
The largest of the two ($\sigma_2=2 \sqrt{2}$) is also the $l_2$-norm of $A$.\\[0.3cm]
In order to find $U$, we can decompose $AA^T$ in the same fashion; however, since we have figured out $\Lambda$ and $V$, an easier way of finding $U$ would be recognizing that:
\begin{equation}\begin{split} 
Av_1 &= \begin{bmatrix} 2 & -2 \\ 1 & 1\end{bmatrix}\begin{bmatrix} \frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2\end{bmatrix} = \begin{bmatrix} 0 \\ \sqrt{2}\end{bmatrix}\\
&=\sqrt{2}\,u_1,\\
Av_2 &= \begin{bmatrix} 2 & -2 \\ 1 & 1\end{bmatrix}\begin{bmatrix} \frac{-\sqrt{2}}2 \\ \frac{\sqrt{2}}2\end{bmatrix} = \begin{bmatrix}-2\sqrt{2}\\0\end{bmatrix}\\
&=2\sqrt{2}\,u_2.
\end{split}\nonumber\end{equation} 
Hence 
\begin{equation}\begin{split} U = \begin{bmatrix} 0 & -1 \\ 1 & 0\end{bmatrix} \end{split}\nonumber\end{equation} and
\begin{equation}\begin{split} A = U\Sigma V^T = U\Lambda^{\frac12}V^T\end{split},\nonumber\end{equation}
where $\Sigma$ is defined as a diagonal matrix with diagonal entries $\Sigma_{ii} = \sigma_i$.\\[0.3cm]

Indeed, we find that 
\begin{equation}\begin{split} 
AV &= \begin{bmatrix} 2 & -2\\1 & 1\end{bmatrix}\begin{bmatrix} \frac{\sqrt{2}}2 & -\frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2 & \frac{\sqrt{2}}2 \end{bmatrix} = 
\begin{bmatrix} 0 & -\sqrt{2} \\ \sqrt{2} & 0\end{bmatrix}\\
&= U\Sigma = \begin{bmatrix} 0 & -1\\1 & 0\end{bmatrix}\begin{bmatrix} \sqrt{2} & 0\\0 & 2 \sqrt{2} \end{bmatrix}   
\end{split}\nonumber\end{equation} 
and that
\begin{equation}\begin{split} 
A^TU &= \begin{bmatrix} 2 &1 \\ -2 & 1\end{bmatrix} \begin{bmatrix} 0 & -1\\1 & 0\end{bmatrix} = \begin{bmatrix} 1 & -2\\ 1 & 2\end{bmatrix} \\
&=V\Sigma = \begin{bmatrix} \frac{\sqrt{2}}2 & -\frac{\sqrt{2}}2 \\ \frac{\sqrt{2}}2 & \frac{\sqrt{2}}2 \end{bmatrix} \begin{bmatrix}\sqrt{2} & 0\\0 & 2\sqrt{2}\end{bmatrix}.
\end{split}\nonumber\end{equation} 

\section{}
\subsection{}
After playing around in \texttt{MATLAB}, (code can be found in Appendix B,) my finding is that \\[0.4cm] 
\textbf{Theorem} Given a symmetric matrix $A$, $(\lambda, v)$ is an eigenpair of $A^TA=A^2$ if and only if $(\lambda^2, v)$ is an eigenpair of $A^T=A$. \\[0.4cm]
\subsection{}
\textit{Proof:}\\[0.4cm]
If 
\begin{equation}\begin{split} Av = \lambda v, \end{split}\nonumber\end{equation}
then
\begin{equation}\begin{split} A^2v=A(Av)=\lambda Av = \lambda^2v;\end{split}\nonumber\end{equation}
in other words, $Av=\lambda v$ only if $A^2v = \lambda^2v$.\\[0.3cm]
On the other hand, $A^2=A^TA$ is symmetric positive semidefinite, so it can be decomposed as
\begin{equation}\begin{split} A^2 = Q\Lambda Q^T = (Q\Lambda^{\frac12}Q^T)(Q\Lambda^{\frac12}Q^T)\end{split},\nonumber\end{equation}
with $Q$ being orthogonal
% TODO: finish the proof.

\subsection{}
Since $$\norm{A}_2 = \sqrt{\max\sigma(A^TA)}$$ and $$\lambda_{A} = \pm\sqrt{\lambda_{A^TA}},$$ the $l_2$-norm of a real symmetric matrix $A$ is equal to the element in its spectrum with the largest absolute vaue, \textit{i.e.} $$\norm{A}_2 = \max_{\lambda}\abs{\lambda},\,\,\lambda\in\sigma(A).$$ 
Apply this result to $$A = \begin{bmatrix} -8 & 144\\144 & -92\end{bmatrix}, $$ 
whose eigenvalues are $$\lambda_1 = -200,\,\,\lambda_2 = 100,$$
we predict the $l_2$-norm of $A$ to be 
$$\norm{A}_2 = \max\{\abs{-200},\,\abs{100}\} = 200,$$
which is indeed the $l_2$-norm of $A$. (Can be verified with \texttt{MATLAB}.)

\subsection{}
The theorem from preceeding parts does not hold for more general matrices; for example, given 
\begin{equation}\begin{split} A &= \begin{bmatrix} 0 & -2 \\ 1 & 0\end{bmatrix},\\
\sigma(A) &= \{-\sqrt{2} ,\, \sqrt{2} \};
  \end{split}\nonumber\end{equation} 
following the previous theorem we would predict that 
$$\norm{A}_2 = \sqrt{2}. $$
However, if we calculate the $l_2$-norm in the canonical way, we would find that 
\begin{equation}\begin{split} \sigma(A^TA) &= \{1,\,4\},\\ 
\norm{A}_2 &= \sqrt{4} = 2\\&\neq\sqrt{2},  
\end{split}\nonumber\end{equation}
which contradicts the theorem. 
\subsection{}
Following the discussion above, we can say that the sigular values of a symmetric matrix are equal to the absolute values of its eigenvalues.

\section{}
\subsection{}
If $A$ is a non-singular matrix and $(\lambda, v)$ is one of its eigenpairs, \textit{i.e.}
\begin{equation}\begin{split} Av = \lambda v, \end{split}\nonumber\end{equation} then 
\begin{equation}\begin{split} A^{-1}Av &= (A^{-1}A)v = v \\ &= A^{-1}(Av) = \lambda A^{-1}v \end{split}\nonumber\end{equation}
$$\Rightarrow A^{-1}v = \frac1{\lambda}v.$$
In addition, $A$ is also the inverse of $A^{-1}$, so $A^{-1}v=\frac1{\lambda}v$ will lead to $Av = \lambda v$ as well. Hence we proved that $Av=\lambda v$ if and only if $A^{-1 }v = \frac1{\lambda}v$.

\subsection{}
Given that $A$ is non-singular, 
$$\norm{A^{-1}}_2 =\frac1{\min\{\sigma\}},\,\sigma^2\in\sigma(A^TA).$$
\textit{Proof:}\\[0.3cm]
Given an SVD of $A$:
$$A = U\Sigma V^T,$$ then
$$A^{-1} = V\Sigma^{-1}U^T,$$
where $(\Sigma^{-1})_{ij} = 0$ if $i\neq j$ and $(\Sigma^{-1})_{ii} = \frac1{\Sigma_{ii}}$.\\[0.3cm]
It can easily be verified that $\Sigma^{-1}$ and $A^{-1}$ expressed as such are exactly the inverses of $\Sigma$ and $A$ respectively, and thus $\{\sigma_i = \Sigma_{ii}\}$ and $\{\sigma^{-1}_i = \Sigma^{-1}_{ii}\}$ are the sets of sigular values of $A$ and $A^{-1}$. The $l_2$-norm of $A^{-1}$ can be expressed as the following:
$$\norm{A^{-1}}_2 = \max_{\sigma^{-1}}{\abs{\sigma^{-1}_i}} = \max_{\sigma_i}\frac1{\sigma_i}=\frac1{\min\{\sigma_i\}}.$$
Consider both $\{\sigma\}$ and $\{\sigma_i\}$ represent the set of singular values of $A$, we've proved the theorem.

\section{}
$$Z = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6\\ 7 & 8 & 7 \\ 4 & 2 & 3 \\ 4 & 2 & 2\end{bmatrix} $$
Since $Z$ is a $5\times3$ rectangular matrix, it can be QR-decomposed into the product of a $5\times5$ orthogonal matrix $Q$ and a $5\times 3$ upper triangular matrix $R$: $$Z_{5\times3} = Q_{5\times5}R_{5\times3}.$$
The $l_2$-norm (abbreviated as simply norm below) of the first column of $Z$, denoted as $x_1$, is $$\norm{x_1}_2 = \sqrt{1^2+4^2+7^2+4^2+4^2}=\sqrt{98} = 7 \sqrt{2}; $$ thus $$y_1 = \begin{bmatrix} 7\sqrt{2}\\0\\0\\0\\0\end{bmatrix},$$ $$ \norm{x_1-y_1} = \sqrt{196-14\sqrt{2}}\approx13.2741, $$ and $$w_1 = \frac{x_1-y_1}{\norm{x_1-y_1}} = \begin{bmatrix} \frac{1-7 \sqrt{2}}{\sqrt{196-14\sqrt{2}}} \\ \frac4{\sqrt{196-14\sqrt{2}}} \\ \frac7{\sqrt{196-14\sqrt{2}}} \\ \frac4{\sqrt{196-14\sqrt{2}}} \\ \frac4{\sqrt{196-14\sqrt{2}}}\end{bmatrix}\approx \begin{bmatrix} -0.6704 \\ 0.3013 \\ 0.5273 \\ 0.3013 \\ 0.3013\end{bmatrix}. $$
We can construct the first Householder matrix as the following:
\begin{equation}\begin{split} H_1 &= I - 2ww^T\\
&\approx 
\begin{bmatrix} 
0.1010 & 0.4041 & 0.7071 & 0.4041 & 0.4041 \\ 
0.4041 & 0.8184 & -0.3178 & -0.1816 & -0.1816 \\
0.7071 & -0.3178 & 0.4438 & -0.3178 & -0.3178 \\
0.4041 & -0.1816 & -0.3178 & 0.8184 & -0.1816 \\
0.4041 & -0.1816 & -0.3178 & -0.1816 & 0,8184
\end{bmatrix}  
\end{split}\nonumber\end{equation} 
Multiply $Z$ by $H_1$ on the left, we have:
$$H_1Z \approx \begin{bmatrix} 
9.8995 & 9.4954 & 9.6975 \\
0 & 1.6311 & 2.9897 \\
0 & 2.1044 & 1.7320 \\
0 & -1.3689 & -0.0103 \\
0 & -1.3689 & -1.0103
\end{bmatrix} $$
Following this strategy we have:
\begin{equation}\begin{split} 
x_2 &\approx 
\begin{bmatrix} 
9.4954 \\ 1.6311 \\ 2.1044 \\ -1.3689 \\  -1.3689
\end{bmatrix},\,\,
y_2 \approx \begin{bmatrix} 
9.4954 \\ 3.2919 \\ 0\\ 0\\ 0
\end{bmatrix},\,\,
w_2 \approx \begin{bmatrix} 0 \\ -0.5023 \\ 0.6364 \\ -0.4140 \\ -0.4140\end{bmatrix},\\
H_2 &= I - 2w_2w_2^T\\
&\approx 
\begin{bmatrix} 
1 & 0 & 0 & 0 & 0 \\
0 & 0.4955 & 0.6393 & -0.4158 & -0.4158 \\
0 & 0.6393 & 0.1900 & 0.5269 & 0.5269 \\
0 & -0.4158 & 0.5269 & 0.6572 & -0.3428 \\
0 & -0.4158 & 0.5269 & -0.3428 & 0.6572
\end{bmatrix},\\
H_2H_1Z &\approx
\begin{bmatrix} 9.8995 & 9.4952 & 9.6975 \\
0 & 3.2919 & 3.0129 \\
0 & 0 & 1.7026 \\
0 & 0 & 0.0089 \\
0 & 0 & -0.9911 
\end{bmatrix} 
\end{split}\nonumber\end{equation} 
and that 
\begin{equation}\begin{split} 
x_3 &\approx
\begin{bmatrix} 9.6975 \\ 3.0129 \\ 1.7026 \\ 0.0089 \\ -0.9911\end{bmatrix},\,\,
y_3 \approx \begin{bmatrix} 
9.6975 \\ 3.0129 \\ 1.9701 \\ 0 \\ 0
\end{bmatrix},\,\,
w_3 \approx \begin{bmatrix} 0 \\ 0 \\ -0.2606 \\ 0.0086 \\ -0.9654\end{bmatrix}, \\
H_3 &= I - 2w_3w_3^T \approx 
\begin{bmatrix} 
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0.8642 & 0.0045 & -0.5031 \\
0 & 0 & 0.0045 & 0.9999 & 0.0167 \\
0 & 0 & -0.5031 & 0.0167 & -0.8641
\end{bmatrix},\\
R &= H_3H_2 H_1Z \approx
\begin{bmatrix} 
9.8995 & 9.4954 & 9.6975 \\
0 & 3.2919 & 3.0129 \\
0 & 0 & 1.9701 \\
0 & 0 & 0\\
0 & 0 & 0
\end{bmatrix},\\
Q &= (H_3H_2H_1)^T = H_1^TH_2^TH_3^T\\
&\approx
\begin{bmatrix} 
0.1010 & 0.3162 & 0.5420 & 0.3408 & -0.6928 \\
0.4041 & 0.3534 & 0.5162 & -0.5730 & 0.3422\\
0.7071 & 0.3906 & -0.5248 & 0.2684 & 0.0028\\
0.0041 & -0.5580 & 0.3871 & 0.5006 & 0.3534\\
0.4041 & -0.5580 & -0.1204 & -0.4825 & -0.5273
\end{bmatrix}.
\end{split}\nonumber\end{equation} 
And we find that $Z = QR$, just as expected.

\newpage
\appendix{}
\section{\texttt{MATLAB} code that verifies results in Problem 1}
% TODO: matlab code to verify the results in problem 1

\section{Evidence of the theorem in Problem 2}
% TODO: provide evidence to the theorem



\end{document}

\title{CS 513 Assignment 1}
\author{Ruochen Lin}
\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{setspace}

\begin{document}
\maketitle
\section{}
\subsection{}
\subsubsection{}
From linear algebra, we know that to apply a column operation on $B$, we can multiply $B$ by a matrix on its right; similarly, to apply a row operation, we can multiply $B$ on its left. In addition, if the desired outcome of a row operation on $B_0$ is $B_1$, and the matrix corresponding to the operation is $A$, i.e. $AB_0=B_1$, we can know about the structure of $A$ by applying it onto identity matrix $I$. For example, if the desired operation is halving row three and the matrix is $A$, then $$A=AI=\begin{bmatrix} 1\\&1\\& &\frac12\\&&&1\end{bmatrix}.$$Apparently, this conclusion can be generalised to multiple consecutive row (column) operations.\\[0.5cm]
With this in mind, we can write out the matrices corresponding to operations 1 through 7. Note that matrices corresponding to row operations are denoted with $A_i$, and column operations $C_i$.
$$C_1=\begin{bmatrix} 2\\&1\\&&1\\&&&1\end{bmatrix} $$
$$A_2=\begin{bmatrix} 1\\&1\\& &\frac12\\&&&1\end{bmatrix}$$
$$A_3=\begin{bmatrix} 1&&1&\\&1\\&&1\\&&&1\end{bmatrix} $$
$$C_4=\begin{bmatrix} &&&1\\&1\\&&1\\1\end{bmatrix} $$
$$A_5=\begin{bmatrix} 1&-1\\&1\\&-1\\&-1&1\end{bmatrix} $$
$$C_6=\begin{bmatrix} 1\\&1\\&&1\\&&1&\end{bmatrix}$$
$$C_7=\begin{bmatrix} 0&0&0\\1&0&0\\0&1&0\\0&0&1\end{bmatrix} $$
And the resulting matrix is $A_5A_3A_2BC_1C_4C_6C_7$. 
\subsubsection{}
$$A=\begin{bmatrix} 1&-1&\frac12&0\\0&1&0&0\\0&-1&\frac12&0\\0&0&0&1\end{bmatrix} $$
$$C=\begin{bmatrix} 0&0&0\\1&0&0\\0&1&1\\0&0&0\end{bmatrix} $$
\subsection{}
Since A is Hermitian, $A=A'$, where $A'$ refers to the adjoint of $A$. Suppose $x\in\mathbb{R}^m$ is an eigenvector of $A$ with eigenvalue $\lambda$: i.e. $Ax=\lambda x$. On one hand, $$(x,Ax)=(x, \lambda x) = \lambda(x,x);$$ on the other, $$(x,Ax)=(A'x, x)=(Ax,x)=(\lambda x,x)=\lambda^*(x,x).$$ Since $\lambda(x,x)=\lambda^*(x,x)$ and $x\neq0$, we know $\lambda=\lambda^*$, and hence all eigenvalues of $A$ are real.  \\[0.5cm]
If $Ax=\lambda_xx$ and $Ay=\lambda_yy$, $\lambda_x\neq\lambda_y$, then $(x,Ay) = \lambda_y(x,y)$ and $(x,Ay)=(A'x,y)=(Ax,y)=\lambda_x(x,y)$. Since $\lambda_x\neq\lambda_y$, $(x,y)=0$. 
\section{}
\subsection{}
We know that $\norm{Qx}_2=\norm{x}_2\Leftrightarrow(Qx,Qx)=(x,x)$. In addition, $(Qx, Qx)=(x,Q'Qx)$. Suppose $x$ is an eigenvector of $Q'Q$ with eigenvalue $\lambda$, then $(x,Q'Qx)=(x,\lambda x=\lambda(x,x)$, and hence $\lambda(x,x)=(x,x)$. Since $x\neq0$, we know that $\lambda=1,\,\forall\lambda\in\sigma(Q'Q)$.
\subsection{}
Write $Q$ as $\begin{bmatrix} q_1 & q_2 & \dots &q_m\end{bmatrix}$, then $$Q'Q=\begin{bmatrix} q_1'\\q_2'\\\vdots\\q_m'\end{bmatrix}\begin{bmatrix} q_1&q_2&\dots&q_m\end{bmatrix}; $$ $(Q'Q)_{ij}=(q_i,q_j)$, $(Q'Q)_{ji}=(q_j,q_i)=(q_i,q_j)=(Q'Q)_{ij}$. Hence $Q'Q$ is symmetric.\\[0.5cm]
Since $Q'Q$ is real and symmetric, it can be written as $Q'Q=P\Lambda P'$, with $P$ being orthogonal and $\Lambda$ diagonal with eigenvalues of $Q$ as its entries. In addition, we've proved $1$ is the only eigenvalue of $Q'Q$; so $\Lambda=I$ in this particular case. Insert this into the decomposed form of $Q'Q$, we have $Q'Q=PIP'=PP'=I$. In other words, $Q^{-1}=Q'$; hence we proved $Q$ is orthogonal.
\section{}

\end{document}
